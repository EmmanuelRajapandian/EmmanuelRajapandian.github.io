---
title: "Feedforward Neural Networks, Word Embeddings and Generalization"
collection: projects
urlslug: "feedforward-NN-word-embedding"
type: "Academic"
permalink: /projects/2024-09-18-feedforward-NN-word-embedding.md
contributors: "Emmanuel Rajapandian"
contribution: "Sole contributor."
date: 2024-09-18
teaserurl: 'code-2.webp'
codeurl: 'https://github.com/emmanuelrajapandian/Transformer-Language-Modeling-and-LLMs'
excerpt: 'In this project, I explored training neural networks over text using PyTorch to enhance sentiment classification from a previous project. Initially, I tackled function optimization by implementing the gradient of a quadratic function and optimizing it using stochastic gradient descent (SGD). This involved calculating the best step size to reach the optimum efficiently. Moving on, the main task was to developed a deep averaging network for sentiment analysis using pretrained GloVe word embeddings. By averaging word vectors and employing a feedforward neural network, I experimented with various configurations, such as layer sizes and embedding dimensions, to optimize performance. Additionally, using batching the input data was handled efficiently, ensuring that sentences of varying lengths were padded appropriately. The project achieved an accuracy of 77% on the development set within a reasonable training time. Finally, I addressed data with misspellings by exploring spelling correction and prefix embeddings to improve model robustness against typographical errors. This comprehensive approach provided valuable insights into neural network training and optimization in text classification tasks.'
---

Emmanuel Rajapandian

**Description:**
<p align="justify"> 
In this project, I explored training neural networks over text using PyTorch to enhance sentiment classification from a previous project. Initially, I tackled function optimization by implementing the gradient of a quadratic function and optimizing it using stochastic gradient descent (SGD). This involved calculating the best step size to reach the optimum efficiently. Moving on, the main task was to developed a deep averaging network for sentiment analysis using pretrained GloVe word embeddings. By averaging word vectors and employing a feedforward neural network, I experimented with various configurations, such as layer sizes and embedding dimensions, to optimize performance. Additionally, using batching the input data was handled efficiently, ensuring that sentences of varying lengths were padded appropriately. The project achieved an accuracy of 77% on the development set within a reasonable training time. Finally, I addressed data with misspellings by exploring spelling correction and prefix embeddings to improve model robustness against typographical errors. This comprehensive approach provided valuable insights into neural network training and optimization in text classification tasks. </p>

**Repository:**
![Code](https://github.com/emmanuelrajapandian/Transformer-Language-Modeling-and-LLMs)

**My contribution:**
<p align="justify"> 
Sole contributor.</p>
